<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Aditya Aggarwal</title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="author" content="Aditya Aggarwal" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="/style.css" />
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

</head>



<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <!-- Table for Intro -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <h1>
                Aditya Aggarwal
              </h1>
              <p>I'm a Research Intern at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-india/">Microsoft Research India</a>, in the Technology and Empowerment group, where I work on the applications of Computer Vision and Machine Learning. My primary focus is developing low-cost smartphone based diagnostic solutions for eye diseases.</p>

              <p> 
                Previously, I worked as a Product Engineer at <a href="https://www.gojek.io/">Gojek Tech</a> on the ride-hailing platform. I am also an Open Source Developer, and have worked on multiple projects for the organization <a href="https://robocomp.github.io/web/">Robocomp</a>.
              </p>
              <p>
                I completed my B.Tech with Honors in May 2020, at <a href="https://www.iiit.ac.in/">IIIT Hyderabad</a>. I am fortunate to be advised by <a href="https://scholar.google.com/citations?user=QDuPGHwAAAAJ&hl=en">Prof. K Madhava Krishna</a> at the <a href="https://robotics.iiit.ac.in/"> Robotics Research Center (RRC)</a>. My work was mainly on improving the SLAM trajectory in the textureless environments.
              </p>
              
              <p style="text-align:center">
                <a target="_blank" href="aditya.aggarwal@alumni.iiit.ac.in"> Email</a> &nbsp;/&nbsp;
                <a href="https://github.com/aditya2g">GitHub</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=4WMAu84AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/aditya-aggarwal-2g/"> LinkedIn </a> &nbsp;/&nbsp;
                <a href="/aditya_resume.pdf"> Resume </a> &nbsp;/&nbsp;
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="/images/profile.jpg">
            </td>
          </tr>
        </table>

        <!-- Table for Research Agenda -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h1>Research</h1>
              <p>
                I'm interested in problems involving computer vision, machine learning and robotics with real world applications. My research vision is to enable embodied agents to perceive our dynamic world and make intelligent decisions.
              </p>
            </td>
          </tr>
        </table>
        
        <!-- Table for Research Projects Stuff -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/quovadis-example.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Quo Vadis, Skeleton Action Recognition? </h3>
              Pranay Gupta, Anirudh Thatipelli, <strong>Aditya Aggarwal</strong>, Shubh Maheshwari, Neel Trivedi, Sourav Das, Ravi Kiran Sarvadevabhatla
              <br>
              <strong>International Journal of Computer Vision (IJCV), 2021</strong>
              <br>
              <a href="https://arxiv.org/abs/2007.02072">arxiv</a> /
              <a href="https://skeleton.iiit.ac.in/">project page</a> / 
              <a href="https://github.com/skelemoa/quovadis">code</a> /
              <a href="https://www.youtube.com/watch?v=0Jexam-CDHU">video</a> /
              <a href="/bibtex/quo_vadis_skeleton.bib">bibtex</a> /
              <p>In this work, we study current and upcoming frontiers across the landscape of skeleton-based human action recognition. We benchmark state-of-the-art models on the NTU-120 dataset and provide a multi-layered assessment. To examine skeleton action recognition 'in the wild', we introduce Skeletics-152 and Skeleton-Mimetics datasets. Our results reveal the challenges and domain gap induced by actions 'in the wild' videos.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/rrb_result.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Reconstruct, Rasterize and Backprop: Dense shape and pose estimation from a single image</h3>
              Aniket Pokale, <strong>Aditya Aggarwal</strong>, KM Jatavallabhula, K Madhava Krishna
              <br>
              <strong>CVPR 2020 Workshop on Long Term Visual Localization, Visual Odometry, and Geometric and Learning-Based SLAM </strong>
              <br>
              <a href="https://arxiv.org/abs/2004.12232">arxiv</a> /
              <a href="https://github.com/aniketpokale10/reconstruct_rasterize_backprop">project page</a> / 
              <a href="https://github.com/aniketpokale10/reconstruct_rasterize_backprop">code</a> /
              <a href="https://www.youtube.com/watch?v=AeLDm6UfWfo&t=2024s">virtual presentation</a> /
              <a href="/bibtex/rrb.bib">bibtex</a> /
              <p>In this work, we present a new system to obtain dense object reconstructions along with 6-DoF poses from a single image. We leverage recent advances in differentiable rendering (in particular, rasterization) to close the loop with 3D reconstruction in camera frame. We demonstrate that our approach—dubbed reconstruct, rasterize and backprop (RRB)—achieves significantly lower pose estimation errors compared to prior art, and is able to recover dense object shapes and poses from imagery. We further extend our results to an (offline) setup, where we demonstrate a dense monocular object-centric egomotion estimation system.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/air_intro.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>A principled formulation of integrating objects in Monocular SLAM</h3>
              Aniket Pokale, Dipanjan Das, <strong>Aditya Aggarwal</strong>, Brojeshwar Bhowmick, K Madhava Krishna
              <br>
              <strong>Advances in Robotics (AIR), 2019 </strong>
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3352593.3352664">ACM Proceedings</a> /
              <a href="https://drive.google.com/file/d/1JwrBaySXMHoG9tYF29LrAMZT-1f0UDbz/view?usp=sharing">video</a> /
              <a href="/bibtex/air_slam.bib">bibtex</a> /
              <p>In this paper,  we present a novel edge-based SLAM framework, along with category-level models, to localize objects in the scene as well as improve the camera trajectory. We integrate object category models in the core SLAM back-end to jointly optimize for the camera trajectory, object poses along with its shape and 3D structure. We show that our joint optimization is able to recover a better camera trajectory than Edge SLAM.</p>
            </td>
          </tr>

        </table>

  
        <!-- Table for Open Source Contributions  -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h1>Open Source Contributions</h1>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/gsoc.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <p>I have been actively contributing to the open-source organization - <a href="https://robocomp.github.io/web/">Robocomp</a> for the past 3 years now.
                      I have mentored multiple projects for Google Summer of Code (GSoC) 2021 and 2020. My proposal was also selected as a project for GSoC 2019 Program.
                      <br><br>
                      <strong> Mentoring and Reviewing </strong>
                      <br>
                      <a href="https://summerofcode.withgoogle.com/projects/#6393334014148608">
                      <papertitle>Sign Languare Recognition</papertitle>
                      </a>
                       - GSoC 2021
                      <br>
                      <a href="https://summerofcode.withgoogle.com/archive/2020/projects/5310102854172672/">
                      <papertitle>Hand Gesture Recognition</papertitle>
                      </a>
                       - GSoC 2020
                      <br>
                      <a href="https://summerofcode.withgoogle.com/archive/2020/projects/4939459423895552/">
                      <papertitle>Human recognition (identification) using multi-modal perception system </papertitle>
                      </a>
                      - GSoC 2020
                      <br><br>
                      <strong> Selected Proposal </strong>
                      <br>
                      Implemented a <a href="https://summerofcode.withgoogle.com/archive/2019/projects/4951617697218560/">
                        <papertitle>People Identification Component</papertitle>
                        </a> for the educational bot - GSoC 2019
                      </p>
                  </td>
                </tr>
              </table>
            </td>
          </tr>
        </table>


        <!-- Table for TAship -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h1>Teaching Experience</h1>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/iiit_logo.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <p> My job as a Teaching Assistant involved conducting regular tutorials, creating and evaluating assignments, paper corrections, and mentoring final course projects. 
                      <br> I have worked as a Teaching Assistant for the following courses:
                      <br>
                      <br> 1. Digital System and Microcontrollers (Fall 2018)
                      <br> 2. Digital Image Processing (Fall 2019)
                      <br> 3. Computer Vision (Spring 2020)
                      </p>
                  </td>
                </tr>
              </table>
            </td>
          </tr>
        </table>


        <!-- Interesting Projects -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h1>Projects</h1>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/cansat.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3>CANSAT 2018</h3>
                    <p>Our team FalconX ranked 24th at the annual CANSAT competition organized by The American Astronautical Society (AAS) at the Stephenville, Texas. The competition simulated a satellite launch in the form of a soft drink can with functioning power, sensors, and communication system alongside a large hen's egg representing a delicate instrument. Launched from an altitude of 700m above launch site, our probe made a series of manoeuvres to descend at a controlled rate, and finally landed without breaking the egg. </p>
                    Check out this <a href="https://blogs.iiit.ac.in/monthly_news/team-falconx-ranks-24th-at-cansat/">blog</a> for more details.
                    <br><br>
                  </td>
                </tr>

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/robocon.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3>ABU Robocon 2018</h3>
                    <p> Check out the problem statement for the contest <a href="https://www.youtube.com/watch?v=Iska65C1kZ8&t=180s">here.</a> 
                    <br> Our team built an autonomus bot using a pneumatic system to throw a shuttlecock tied to a thread through a ring. It also coordinated with a manual bot which picked up the shuttlecock and passed it to the automatic bot.</p>
                    Check this <a href="https://drive.google.com/file/d/12w7Z8-lu1YxrMN_lRbK2GCbOpDmvr_SI/view?usp=sharing">video</a> for a demo.
                    <br><br><br>
                  </td>
                </tr>
      
        </table>

        <!-- Credits -->
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>

