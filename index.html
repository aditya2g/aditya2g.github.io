<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Aditya Aggarwal</title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="author" content="Aditya Aggarwal" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="/style.css" />
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

</head>



<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <!-- Table for Intro -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <h1>
                Aditya Aggarwal
              </h1>
              <p> I am a masters student in the CSE department at <a href="https://cse.ucsd.edu/">UC San Diego</a> since Fall 22. I am currently working at Google in Privacy, Safety and Security as a SWE Intern focusing on reducing the OpEx cost of manual review systems. I am also working as a Graduate Researcher in the <a href="https://www.cogrob.org/#about">Cognitive Robotics Lab</a> on the Home Robot project under the mentorship of <a href="https://www.hichristensen.com/">Prof. Henrik Christensen</a>
              <p>Previously, I was a Research Intern at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-india/">Microsoft Research, India</a>, in the Technology and Empowerment group, where I work closely with <a href="https://www.microsoft.com/en-us/research/people/nkwatra/">Dr. Nipun Kwatra</a> and <a href="https://mohitjaindr.github.io/">Dr. Mohit Jain</a> at the intersection of HCI, Computer Vision and Healthcare. My primary focus was on developing a low-cost smartphone based diagnostic solution for detecting eye diseases. I have also contributed to multiple open-source projects for the organization <a href="https://robocomp.github.io/web/">Robocomp</a>.</p>

              <!-- <p> 
                Before that, I also worked as a Product Engineer at <a href="https://www.gojek.io/">Gojek</a>, a ride-hailing platform. I am an active Open Source Developer, and have contributed to multiple projects for the organization <a href="https://robocomp.github.io/web/">Robocomp</a>.
              </p> -->
              <p>
                I graduated from <a href="https://www.iiit.ac.in/">IIIT Hyderabad</a> in 2020 with a B.Tech (Honors), where I was advised by <a href="https://scholar.google.com/citations?user=QDuPGHwAAAAJ&hl=en">Prof. K Madhava Krishna</a> and <a href="https://ravika.github.io/index.html">Prof. Ravi Kiran Sarvadevabhatla </a>. I worked mainly on vision-guided robot navigation and human activity understanding.
              </p>

              <p> I am currently looking for full-time software engineering / machine learning opportunities starting January 2024, so if you have a position available or just want to say hi, you can always mail me.


              
              <p style="text-align:center">
                <a href="mailto:aditya.aggarwal@alumni.iiit.ac.in">Email</a> &nbsp;/&nbsp;
                <a href="https://github.com/aditya2g">GitHub</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=4WMAu84AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/aditya-aggarwal-2g/"> LinkedIn </a> &nbsp;/&nbsp;
                <a href="/aditya_resume.pdf"> Resume </a> &nbsp;/&nbsp;
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="/images/profile.jpg">
            </td>
          </tr>
        </table>

        <!-- Table for Experience -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h1>Experience</h1>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/google_logo.png" alt="Google Logo" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3>Google </h3>
                    <strong>Software Engineering Intern (June 2023 - September 2023)</strong> <br>
                    <em>Sunnyvale, USA</em>
                    <p> During the internship, I developed a gRPC service and data processing pipeline utilizing MapReduce to seamlessly migrate manual review records from Google Shopping to a centralized database. These records were then used to generate customized analytics and real-time alerts. By leveraging messaging queues, I significantly improved the system's scalability, successfully managing a burst of up to <strong>1000 queries per second (QPS)</strong> and handling a daily influx of <strong>5000 records</strong>. I also implemented an extensible system architecture by introducing support for both Push and Pull frameworks, which remarkably reduced the onboarding time from <strong>8 weeks to a mere 2 weeks</strong>. </p>
                    <p> <Strong>Tech Stack:</Strong> Java, MapReduce, SQL</p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/ms_logo_cam.gif" alt="Microsoft Logo" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3>Microsoft Research </h3>
                    <strong>Research Intern (February 2021 - August 2022)</strong> <br>
                    <em>Bangalore, India</em>
                    <p> I led a project that automated the retinoscopy method by combining a smartphone with a retinoscope. I developed an android app which recorded videos at 120 fps and integrated it with a cloud-deployed video analysis pipeline which estimated refractive error of the eye. The result was a potential real-world medical tool that could simplify and enhance refractive error screening. The project's success was underscored by a clinical evaluation involving <strong>128 patients</strong>, achieving a remarkable SOTA Mean Absolute Error of <strong>0.75 ± 0.67D</strong>.  </p>
                    <p> <Strong>Tech Stack:</Strong> Python, Android SDK, Javascript, React, Flask, SQL</p>
                  </td>
                </tr>

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/gojek_logo.png" alt="Gojek Logo" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3>Gojek </h3>
                    <strong>Product Engineer (June 2020 - January 2021)</strong> <br>
                    <em>Bangalore, India</em>
                    <p> I collaborated on a ride-hailing platform used by 4 million daily users, focusing on improving on-demand features and the booking process. I developed a scheduling service allowing future ride bookings, raising the Booking Conversion Rate from 90% to 95%. Additionally, I used Firebase remote config APIs to enhance system extensibility and fault tolerance across Android and iOS platforms, without requiring app updates. </p>
                    <p> <Strong>Tech Stack:</Strong> Go, Ruby, Kotlin, SQL</p>
                  </td>
                </tr>
              </table>
            </td>
          </tr>
        </table>

        <!-- Table for Research Agenda -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h1>Research</h1>
              <p>
                I'm interested in problems involving computer vision, machine learning and robotics with real world applications. My research vision is to enable embodied agents to perceive our dynamic world and make intelligent decisions.
              </p>
            </td>
          </tr>
        </table>
        
        <!-- Table for Research Projects Stuff -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/auto_retinoscopy.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Towards Automating Retinoscopy for Refractive Error Diagnosis </h3>
              <br>
              <strong>Aditya Aggarwal</strong>, 
              <a href="https://sidgairo18.github.io/">Siddhartha Gairola</a>,
              <a href="https://www.microsoft.com/en-us/research/people/nkwatra/">Nipun Kwatra</a>,
              <a href="https://mohitjaindr.github.io/">Mohit Jain</a>
              <br>
              <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), Volume 6, Issue 3, 2022</em>
              <br>
              <a href="https://arxiv.org/abs/2208.05552">arxiv</a> /
              <a href="https://www.microsoft.com/en-us/research/project/auto-retinoscopy-automating-retinoscopy-for-refractive-error-diagnosis/">project page</a> / 
              <a href="https://github.com/microsoft/Auto-retinoscopy">code</a> /
              <a href="https://youtu.be/XdyXy2NOlvo">virtual presentation</a> /
              <a href="/bibtex/automated_retinoscopy.bib">bibtex</a> /
              <p>Refractive error is the most common eye disorder and is the key cause behind correctable visual impairment. It can be diagnosed using multiple methods, including subjective refraction, retinoscopy, and autorefractors. Retinoscopy is an objective refraction method that does not require any input from the patient. In this work, we automate retinoscopy by attaching a smartphone to a retinoscope and recording retinoscopic videos with the patient wearing a custom pair of paper frames. The results from our video processing pipeline and mathematical model indicate that our approach has the potential to be used as a retinoscopy-based refractive error screening tool in real-world medical settings.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/quovadis-example.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Quo Vadis, Skeleton Action Recognition? </h3>
              <br>
              <a href="https://pranaygupta36.github.io/">Pranay Gupta</a>, 
              <a href="https://anirudh257.github.io//">Anirudh Thatipelli</a>,
              <strong>Aditya Aggarwal</strong>, 
              <a href="https://www.linkedin.com/in/shubh-maheshwari-663737151/?originalSubdomain=in">Shubh Maheshwari</a>,
              <a href="https://www.linkedin.com/in/neel-trivedi-7a486b193/?originalSubdomain=in">Neel Trivedi</a>,
              <a href="https://github.com/SodaCoder">Sourav Das</a>, 
              <a href="https://ravika.github.io/">Ravi Kiran Sarvadevabhatla</a>
              <br>
              <em>International Journal of Computer Vision (IJCV), Special Issue on Human pose, Motion, Activities and Shape in 3D, 2021</em>
              <br>
              <a href="https://arxiv.org/abs/2007.02072">arxiv</a> /
              <a href="https://skeleton.iiit.ac.in/">project page</a> / 
              <a href="https://github.com/skelemoa/quovadis">code</a> /
              <a href="https://www.youtube.com/watch?v=0Jexam-CDHU">video</a> /
              <a href="/bibtex/quo_vadis_skeleton.bib">bibtex</a> /
              <p>In this work, we study current and upcoming frontiers across the landscape of skeleton-based human action recognition. We benchmark state-of-the-art models on the NTU-120 dataset and provide a multi-layered assessment. To examine skeleton action recognition 'in the wild', we introduce Skeletics-152 and Skeleton-Mimetics datasets. Our results reveal the challenges and domain gap induced by actions 'in the wild' videos.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/rrb_result.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Reconstruct, Rasterize and Backprop: Dense shape and pose estimation from a single image</h3>
              <br>
              <a href="https://www.linkedin.com/in/aniket-pokale-6957689b/">Aniket Pokale*</a>, 
              <strong>Aditya Aggarwal*</strong>, 
              <a href="https://krrish94.github.io/">KM Jatavallabhula</a>,
              <a href="https://robotics.iiit.ac.in/">K Madhava Krishna</a>
              <br>
              <em>CVPR Workshop on Long Term Visual Localization, Visual Odometry, and Geometric and Learning-Based SLAM, 2020</em>
              <br>
              <a href="https://arxiv.org/abs/2004.12232">arxiv</a> /
              <a href="https://github.com/aniketpokale10/reconstruct_rasterize_backprop">project page</a> / 
              <a href="https://github.com/aniketpokale10/reconstruct_rasterize_backprop">code</a> /
              <a href="https://www.youtube.com/watch?v=AeLDm6UfWfo&t=2024s">virtual presentation</a> /
              <a href="/bibtex/rrb.bib">bibtex</a> /
              <p>In this work, we present a new system to obtain dense object reconstructions along with 6-DoF poses from a single image. We demonstrate that our approach—dubbed reconstruct, rasterize and backprop (RRB)—achieves significantly lower pose estimation errors compared to prior art, and is able to recover dense object shapes and poses from imagery. We further extend our results to an (offline) setup, where we demonstrate a dense monocular object-centric egomotion estimation system.</p>
              * Both authors contributed equally towards this work.
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/air_intro.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>A principled formulation of integrating objects in Monocular SLAM</h3>
              <br>
              <a href="https://www.linkedin.com/in/aniket-pokale-6957689b/">Aniket Pokale</a>, 
              Dipanjan Das, 
              <strong>Aditya Aggarwal</strong>, 
              Brojeshwar Bhowmick, 
              <a href="https://robotics.iiit.ac.in/">K Madhava Krishna</a>
              <br>
              <em>Advances in Robotics (AIR), 2019 </em>
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3352593.3352664">ACM Proceedings</a> /
              <a href="https://drive.google.com/file/d/1JwrBaySXMHoG9tYF29LrAMZT-1f0UDbz/view?usp=sharing">video</a> /
              <a href="/bibtex/air_slam.bib">bibtex</a> /
              <p>In this paper,  we present a novel edge-based SLAM framework, along with category-level models, to localize objects in the scene as well as improve the camera trajectory. We integrate object category models in the core SLAM back-end to jointly optimize for the camera trajectory, object poses along with its shape and 3D structure. We show that our joint optimization is able to recover a better camera trajectory than Edge SLAM.</p>
            </td>
          </tr>

        </table>

  
        <!-- Table for Open Source Contributions  -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h1>Open Source Contributions</h1>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/gsoc.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <p>I have also contributed to the open-source organization - <a href="https://robocomp.github.io/web/">Robocomp</a> for past three years.
                      I have mentored multiple projects for Google Summer of Code (GSoC) 2021 and 2020. My proposal was also selected as a project for GSoC 2019 Program.
                      <br><br>
                      <strong> Mentoring and Reviewing </strong>
                      <br>
                      <a href="https://summerofcode.withgoogle.com/projects/#6393334014148608">
                      <papertitle>Sign Languare Recognition</papertitle>
                      </a>
                       - GSoC 2021
                      <br>
                      <a href="https://summerofcode.withgoogle.com/archive/2020/projects/5310102854172672/">
                      <papertitle>Hand Gesture Recognition</papertitle>
                      </a>
                       - GSoC 2020
                      <br>
                      <a href="https://summerofcode.withgoogle.com/archive/2020/projects/4939459423895552/">
                      <papertitle>Human recognition (identification) using multi-modal perception system </papertitle>
                      </a>
                      - GSoC 2020
                      <br><br>
                      <strong> Selected Proposal </strong>
                      <br>
                      Implemented a <a href="https://summerofcode.withgoogle.com/archive/2019/projects/4951617697218560/">
                        <papertitle>People Identification Component</papertitle>
                        </a> for the educational bot - GSoC 2019
                      </p>
                  </td>
                </tr>
              </table>
            </td>
          </tr>
        </table>


        <!-- Table for Education -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h1>Education</h1>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/ucsd_logo.png" alt="UCSD" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3> UC San Diego</h3>
                    Masters in Computer Science and Engineering <strong> CGPA: 3.9 / 4 </strong> <br>
                    San Diego, CA (2022 - Present) <br>
                    <Strong>Graduate Researcher at Cognitive Robotics Lab</Strong><br>
                  </td>
                </tr>

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/iiit_logo.png" alt="IIIT Hyderabad" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3> IIIT Hyderabad</h3>
                    B. Tech with Honors in Electronics and Communication Engineering <strong> CGPA:   8.9 / 10 </strong> <br>
                    Hyderabad, India (2016 - 2020) <br>
                    <Strong>Undergraduate Researcher at CVIT Lab, Robotics Research Center </Strong><br>
                  </td>
                </tr>
              </table>
            </td>
          </tr>
        </table>


        <!-- Interesting Projects -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h1>Projects</h1>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                
                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/drones.JPG" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3>Fault tolerant distributed system</h3>
                    <p>Built a distributed file storage system using gRPC, with concurrent reads and write support. Implemented consistent hashing and RAFT consensus protocol to make the system scalable and resilient to failure of up to 50% of servers</p>
                    Check out the code <a href="https://github.com/ucsd-cse224-wi23/proj5-aditya2g">here</a>.
                    <br>
                  </td>
                </tr>
                
                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/drones.JPG" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3>Mosquitoes vs Drones</h3>
                    <p>Developed an end-to-end system which identified possible mosquito breeding grounds (water logged areas) in aerial images. I also implemented a path planning algorithm for drone to reach the predicted location and destroy the mosquito larvae.</p>
                    Check out the entire workflow in this <a href="https://drive.google.com/file/d/17nAaNSt_XW65q0_nkLbzNSybbtfxC3bp/view">video</a>.
                    <br>
                  </td>
                </tr>

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/cansat.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3>CANSAT 2018</h3>
                    <p>Our team FalconX ranked 24th at the annual CANSAT competition organized by The American Astronautical Society (AAS) at the Stephenville, Texas. The competition simulated a satellite launch in the form of a soft drink can with functioning power, sensors, and communication system alongside a large hen's egg representing a delicate instrument. Launched from an altitude of 700m above launch site, our probe made a series of manoeuvres to descend at a controlled rate, and finally landed without breaking the egg. </p>
                    Check out this <a href="https://blogs.iiit.ac.in/monthly_news/team-falconx-ranks-24th-at-cansat/">blog</a> for more details.
                    <br><br>
                  </td>
                </tr>

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/robocon.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3>ABU Robocon 2018</h3>
                    <p> Check out the problem statement for the contest <a href="https://www.youtube.com/watch?v=Iska65C1kZ8&t=180s">here.</a> 
                    <br> Our team built an autonomus bot using a pneumatic system to throw a shuttlecock tied to a thread through a ring. It also coordinated with a manual bot which picked up the shuttlecock and passed it to the automatic bot.</p>
                    Check this <a href="https://drive.google.com/file/d/12w7Z8-lu1YxrMN_lRbK2GCbOpDmvr_SI/view?usp=sharing">video</a> for a demo.
                    <br>
                  </td>
                </tr>

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/game_of_life.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3>Game of Life</h3>
                    <p>Implemented a popular zero player simulation game showing the cellular automation. It takes an input configuration and the user can observe how it evolves over time. I followed MVC architecture and TDD paradigm while working on this project. </p>
                    Check out this <a href="https://github.com/aditya2g/game-of-life/">repository</a> for code implementation in Java and Ruby.
                    <br><br>
                  </td>
                </tr>

      
        </table>

        <!-- Credits -->
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>

