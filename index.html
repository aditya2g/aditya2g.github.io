<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Aditya Aggarwal</title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="author" content="Aditya Aggarwal" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="/style.css" />
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

</head>



<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <!-- Table for Intro -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <h1>
                Aditya Aggarwal
              </h1>
              <p> I am an incoming masters student in the CSE department at <a href="https://cse.ucsd.edu/">UCSD</a>.
              <p>Previously, I was a Research Intern at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-india/">Microsoft Research, India</a>, in the Technology and Empowerment group, where I work closely with <a href="https://www.microsoft.com/en-us/research/people/nkwatra/">Dr. Nipun Kwatra</a> and <a href="https://mohitjaindr.github.io/">Dr. Mohit Jain</a> at the intersection of HCI, Computer Vision and Healthcare. My primary focus is on developing a low-cost smartphone based diagnostic solution for eye diseases.</p>

              <p> 
                Earlier, I worked as a Product Engineer at <a href="https://www.gojek.io/">Gojek Tech</a> on the ride-hailing platform. I am an active Open Source Developer, and have contributed to multiple projects for the organization <a href="https://robocomp.github.io/web/">Robocomp</a>.
              </p>
              <p>
                I graduated from <a href="https://www.iiit.ac.in/">IIIT Hyderabad</a> in 2020 with a B.Tech (Honors), where I was advised by <a href="https://scholar.google.com/citations?user=QDuPGHwAAAAJ&hl=en">Prof. K Madhava Krishna</a> and <a href="https://ravika.github.io/index.html">Prof. Ravi Kiran Sarvadevabhatla </a>. I worked mainly on vision-guided robot navigation and human activity understanding.
              </p>
              
              <p style="text-align:center">
                <a href="mailto:aditya.aggarwal@alumni.iiit.ac.in">Email</a> &nbsp;/&nbsp;
                <a href="https://github.com/aditya2g">GitHub</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=4WMAu84AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/aditya-aggarwal-2g/"> LinkedIn </a> &nbsp;/&nbsp;
                <a href="/aditya_resume.pdf"> Resume </a> &nbsp;/&nbsp;
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="/images/profile.jpg">
            </td>
          </tr>
        </table>

        <!-- Table for Research Agenda -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h1>Research</h1>
              <p>
                I'm interested in problems involving computer vision, machine learning and robotics with real world applications. My research vision is to enable embodied agents to perceive our dynamic world and make intelligent decisions.
              </p>
            </td>
          </tr>
        </table>
        
        <!-- Table for Research Projects Stuff -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/auto_retinoscopy.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Towards Automating Retinoscopy for Refractive Error Diagnosis </h3>
              <br>
              <strong>Aditya Aggarwal</strong>, 
              <a href="https://sidgairo18.github.io/">Siddhartha Gairola</a>,
              <a href="https://www.microsoft.com/en-us/research/people/nkwatra/">Nipun Kwatra</a>,
              <a href="https://mohitjaindr.github.io/">Mohit Jain</a>
              <br>
              <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), Volume 6, Issue 3, 2022</em>
              <br>
              <a href="https://arxiv.org/abs/2208.05552">arxiv</a> /
              <a href="https://www.microsoft.com/en-us/research/project/auto-retinoscopy-automating-retinoscopy-for-refractive-error-diagnosis/">project page</a> / 
              <a href="https://github.com/microsoft/Auto-retinoscopy">code</a> /
              <p>Refractive error is the most common eye disorder and is the key cause behind correctable visual impairment. It can be diagnosed using multiple methods, including subjective refraction, retinoscopy, and autorefractors. Retinoscopy is an objective refraction method that does not require any input from the patient. In this work, we automate retinoscopy by attaching a smartphone to a retinoscope and recording retinoscopic videos with the patient wearing a custom pair of paper frames. The results from our video processing pipeline and mathematical model indicate that our approach has the potential to be used as a retinoscopy-based refractive error screening tool in real-world medical settings.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/quovadis-example.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Quo Vadis, Skeleton Action Recognition? </h3>
              <br>
              <a href="https://pranaygupta36.github.io/">Pranay Gupta</a>, 
              <a href="https://anirudh257.github.io//">Anirudh Thatipelli</a>,
              <strong>Aditya Aggarwal</strong>, 
              <a href="https://www.linkedin.com/in/shubh-maheshwari-663737151/?originalSubdomain=in">Shubh Maheshwari</a>,
              <a href="https://www.linkedin.com/in/neel-trivedi-7a486b193/?originalSubdomain=in">Neel Trivedi</a>,
              <a href="https://github.com/SodaCoder">Sourav Das</a>, 
              <a href="https://ravika.github.io/">Ravi Kiran Sarvadevabhatla</a>
              <br>
              <em>International Journal of Computer Vision (IJCV), Special Issue on Human pose, Motion, Activities and Shape in 3D, 2021</em>
              <br>
              <a href="https://arxiv.org/abs/2007.02072">arxiv</a> /
              <a href="https://skeleton.iiit.ac.in/">project page</a> / 
              <a href="https://github.com/skelemoa/quovadis">code</a> /
              <a href="https://www.youtube.com/watch?v=0Jexam-CDHU">video</a> /
              <a href="/bibtex/quo_vadis_skeleton.bib">bibtex</a> /
              <p>In this work, we study current and upcoming frontiers across the landscape of skeleton-based human action recognition. We benchmark state-of-the-art models on the NTU-120 dataset and provide a multi-layered assessment. To examine skeleton action recognition 'in the wild', we introduce Skeletics-152 and Skeleton-Mimetics datasets. Our results reveal the challenges and domain gap induced by actions 'in the wild' videos.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/rrb_result.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Reconstruct, Rasterize and Backprop: Dense shape and pose estimation from a single image</h3>
              <br>
              <a href="https://www.linkedin.com/in/aniket-pokale-6957689b/">Aniket Pokale*</a>, 
              <strong>Aditya Aggarwal*</strong>, 
              <a href="https://krrish94.github.io/">KM Jatavallabhula</a>,
              <a href="https://robotics.iiit.ac.in/">K Madhava Krishna</a>
              <br>
              <em>CVPR Workshop on Long Term Visual Localization, Visual Odometry, and Geometric and Learning-Based SLAM, 2020</em>
              <br>
              <a href="https://arxiv.org/abs/2004.12232">arxiv</a> /
              <a href="https://github.com/aniketpokale10/reconstruct_rasterize_backprop">project page</a> / 
              <a href="https://github.com/aniketpokale10/reconstruct_rasterize_backprop">code</a> /
              <a href="https://www.youtube.com/watch?v=AeLDm6UfWfo&t=2024s">virtual presentation</a> /
              <a href="/bibtex/rrb.bib">bibtex</a> /
              <p>In this work, we present a new system to obtain dense object reconstructions along with 6-DoF poses from a single image. We demonstrate that our approach—dubbed reconstruct, rasterize and backprop (RRB)—achieves significantly lower pose estimation errors compared to prior art, and is able to recover dense object shapes and poses from imagery. We further extend our results to an (offline) setup, where we demonstrate a dense monocular object-centric egomotion estimation system.</p>
              * Both authors contributed equally towards this work.
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/images/air_intro.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>A principled formulation of integrating objects in Monocular SLAM</h3>
              <br>
              <a href="https://www.linkedin.com/in/aniket-pokale-6957689b/">Aniket Pokale</a>, 
              Dipanjan Das, 
              <strong>Aditya Aggarwal</strong>, 
              Brojeshwar Bhowmick, 
              <a href="https://robotics.iiit.ac.in/">K Madhava Krishna</a>
              <br>
              <em>Advances in Robotics (AIR), 2019 </em>
              <br>
              <a href="https://dl.acm.org/doi/abs/10.1145/3352593.3352664">ACM Proceedings</a> /
              <a href="https://drive.google.com/file/d/1JwrBaySXMHoG9tYF29LrAMZT-1f0UDbz/view?usp=sharing">video</a> /
              <a href="/bibtex/air_slam.bib">bibtex</a> /
              <p>In this paper,  we present a novel edge-based SLAM framework, along with category-level models, to localize objects in the scene as well as improve the camera trajectory. We integrate object category models in the core SLAM back-end to jointly optimize for the camera trajectory, object poses along with its shape and 3D structure. We show that our joint optimization is able to recover a better camera trajectory than Edge SLAM.</p>
            </td>
          </tr>

        </table>

  
        <!-- Table for Open Source Contributions  -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h1>Open Source Contributions</h1>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/gsoc.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <p>I have been actively contributing to the open-source organization - <a href="https://robocomp.github.io/web/">Robocomp</a> for the past 3 years now.
                      I have mentored multiple projects for Google Summer of Code (GSoC) 2021 and 2020. My proposal was also selected as a project for GSoC 2019 Program.
                      <br><br>
                      <strong> Mentoring and Reviewing </strong>
                      <br>
                      <a href="https://summerofcode.withgoogle.com/projects/#6393334014148608">
                      <papertitle>Sign Languare Recognition</papertitle>
                      </a>
                       - GSoC 2021
                      <br>
                      <a href="https://summerofcode.withgoogle.com/archive/2020/projects/5310102854172672/">
                      <papertitle>Hand Gesture Recognition</papertitle>
                      </a>
                       - GSoC 2020
                      <br>
                      <a href="https://summerofcode.withgoogle.com/archive/2020/projects/4939459423895552/">
                      <papertitle>Human recognition (identification) using multi-modal perception system </papertitle>
                      </a>
                      - GSoC 2020
                      <br><br>
                      <strong> Selected Proposal </strong>
                      <br>
                      Implemented a <a href="https://summerofcode.withgoogle.com/archive/2019/projects/4951617697218560/">
                        <papertitle>People Identification Component</papertitle>
                        </a> for the educational bot - GSoC 2019
                      </p>
                  </td>
                </tr>
              </table>
            </td>
          </tr>
        </table>


        <!-- Table for TAship -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h1>Teaching Experience</h1>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/iiit_logo.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <p> My job as a Teaching Assistant involved conducting regular tutorials, creating and evaluating assignments, paper corrections, and mentoring final course projects. 
                      <br> I have worked as a Teaching Assistant for the following courses:
                      <br>
                      <br> 1. Computer Vision (Spring 2020)
                      <br> 2. Digital Image Processing (Fall 2019)
                      <br> 3. Digital System and Microcontrollers (Fall 2018)
                      </p>
                  </td>
                </tr>
              </table>
            </td>
          </tr>
        </table>


        <!-- Interesting Projects -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h1>Projects</h1>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                
                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/drones.JPG" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3>Mosquitoes vs Drones</h3>
                    <p>Developed an end-to-end system which identified possible mosquito breeding grounds (water logged areas) in aerial images. I also implemented a path planning algorithm for drone to reach the predicted location and destroy the mosquito larvae.</p>
                    Check out the entire workflow in this <a href="https://drive.google.com/file/d/17nAaNSt_XW65q0_nkLbzNSybbtfxC3bp/view">video</a>.
                    <br>
                  </td>
                </tr>

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/cansat.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3>CANSAT 2018</h3>
                    <p>Our team FalconX ranked 24th at the annual CANSAT competition organized by The American Astronautical Society (AAS) at the Stephenville, Texas. The competition simulated a satellite launch in the form of a soft drink can with functioning power, sensors, and communication system alongside a large hen's egg representing a delicate instrument. Launched from an altitude of 700m above launch site, our probe made a series of manoeuvres to descend at a controlled rate, and finally landed without breaking the egg. </p>
                    Check out this <a href="https://blogs.iiit.ac.in/monthly_news/team-falconx-ranks-24th-at-cansat/">blog</a> for more details.
                    <br><br>
                  </td>
                </tr>

                <tr>
                  <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
                    <img src="/images/robocon.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
                  </td>
                  <td style="padding:2.5%;width:75%;vertical-align:middle">
                    <h3>ABU Robocon 2018</h3>
                    <p> Check out the problem statement for the contest <a href="https://www.youtube.com/watch?v=Iska65C1kZ8&t=180s">here.</a> 
                    <br> Our team built an autonomus bot using a pneumatic system to throw a shuttlecock tied to a thread through a ring. It also coordinated with a manual bot which picked up the shuttlecock and passed it to the automatic bot.</p>
                    Check this <a href="https://drive.google.com/file/d/12w7Z8-lu1YxrMN_lRbK2GCbOpDmvr_SI/view?usp=sharing">video</a> for a demo.
                    <br>
                  </td>
                </tr>
      
        </table>

        <!-- Credits -->
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>

